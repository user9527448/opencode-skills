---
name: code-review-guardian
description: OWASP-aligned comprehensive code review - security, correctness, performance, maintainability, testing, documentation, architecture, concurrency
license: MIT
compatibility: opencode
metadata:
  references:
    dimensions: references/dimensions/
    examples: examples/
    templates: templates/
    scripts: scripts/
---

# Code Review Guardian

Review code like a senior engineer. Security first, always. Output standardized reports.

---

## ğŸš¨ When to Activate This Skill

| Trigger | Priority |
|---------|----------|
| Pull request review | HIGH |
| Pre-merge check | HIGH |
| Security audit | CRITICAL |
| Post-implementation review | MEDIUM |
| Code quality check | MEDIUM |

---

## ğŸ“‹ Execution Workflow

### Step 1: Scope Definition
```
1. Identify files changed (git diff, PR files)
2. Determine review depth (quick/full/security-only)
3. Note language/framework for context
```

### Step 2: Automated Scans (Use Tools)
```
1. LSP Diagnostics â†’ Type errors, linting issues
2. AST-grep â†’ Pattern-based security/quality checks
3. Grep â†’ Find TODOs, FIXMEs, hardcoded values
4. Run auto-scan.py â†’ Quick security scan
```

### Step 3: Manual Review (By Dimension)
```
Review in order: Security â†’ Correctness â†’ Architecture â†’ Performance 
                 â†’ Maintainability â†’ Concurrency â†’ Testing â†’ Documentation
                 
Use references/dimensions/ for detailed guidance on each dimension
```

### Step 4: Generate Report
```
Use templates/report-template.md for standardized output
```

---

## Review Dimensions (Critical â†’ Nice-to-have)

```
1. ğŸ”’ Security      â†’ ALWAYS FIRST
2. ğŸ¯ Correctness   â†’ Does it work?
3. ğŸ—ï¸ Architecture  â†’ Design patterns, SOLID
4. âš¡ Performance    â†’ Any bottlenecks?
5. ğŸ§¹ Maintainability â†’ Readable? DRY?
6. ğŸ”„ Concurrency   â†’ Thread/process safe?
7. â™¿ Accessibility â†’ Frontend a11y
8. ğŸ§ª Testing       â†’ Covered?
9. ğŸ“š Documentation â†’ Updated?
```

---

## Quick Reference Card

```
ğŸ“‹ EXECUTION ORDER
1. Scope â†’ 2. Auto-scan â†’ 3. Manual review â†’ 4. Report

ğŸ”’ SECURITY    â†’ First, always
   â–¡ Injection risks (SQL, CMD, XSS)
   â–¡ Auth/authz gaps
   â–¡ Secrets in code
   â–¡ Input validation

ğŸ¯ CORRECTNESS â†’ Works as expected?
   â–¡ Edge cases (null, empty, boundary)
   â–¡ Error handling
   â–¡ Type safety

ğŸ—ï¸ ARCHITECTURE â†’ Well designed?
   â–¡ SOLID principles
   â–¡ No circular deps
   â–¡ Separation of concerns

âš¡ PERFORMANCE  â†’ Scalable?
   â–¡ N+1 queries
   â–¡ Memory leaks
   â–¡ Blocking I/O

ğŸ§¹ MAINTAINABILITY â†’ Readable?
   â–¡ Naming clarity
   â–¡ Complexity < 10
   â–¡ DRY principle

ğŸ”„ CONCURRENCY â†’ Thread safe?
   â–¡ Race conditions
   â–¡ Proper locking

â™¿ ACCESSIBILITY â†’ A11y compliant?
   â–¡ Alt text
   â–¡ Keyboard nav
   â–¡ Color contrast

ğŸ§ª TESTING     â†’ Covered?
   â–¡ New tests for new code
   â–¡ Edge cases tested
   â–¡ All tests pass

ğŸ“š DOCS        â†’ Updated?
   â–¡ API documentation
   â–¡ README updates
   â–¡ Breaking changes noted

ğŸ“Š OUTPUT â†’ Use templates/report-template.md
```

---

## ğŸ“ Directory Structure

```
code-review-guardian/
â”œâ”€â”€ SKILL.md
â”œâ”€â”€ references/
â”‚   â””â”€â”€ dimensions/
â”‚       â”œâ”€â”€ security.md        # OWASP Top 10, vulnerability patterns
â”‚       â”œâ”€â”€ correctness.md     # Logic verification, error handling
â”‚       â”œâ”€â”€ architecture.md    # SOLID principles, design patterns
â”‚       â”œâ”€â”€ performance.md     # N+1, memory, algorithms
â”‚       â”œâ”€â”€ maintainability.md # Naming, complexity, code smells
â”‚       â”œâ”€â”€ concurrency.md     # Thread safety, race conditions
â”‚       â”œâ”€â”€ accessibility.md   # WCAG 2.1, ARIA
â”‚       â”œâ”€â”€ testing.md         # Test quality, patterns
â”‚       â””â”€â”€ documentation.md   # JSDoc, comments
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ scenarios/
â”‚       â”œâ”€â”€ rest-api-review.md    # API review scenario
â”‚       â””â”€â”€ frontend-component.md  # React component scenario
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ report-template.md     # Standard review report format
â”‚   â””â”€â”€ checklist-all.md       # Complete review checklist
â””â”€â”€ scripts/
    â””â”€â”€ auto-scan.py          # Automated security scanner
```

---

## ğŸ“– Reference Files

| Category | Location | Contents |
|----------|----------|----------|
| **Dimensions** | `references/dimensions/` | 9 detailed review dimension guides |
| **Scenarios** | `examples/scenarios/` | Real-world review examples |
| **Templates** | `templates/` | Report template, complete checklist |
| **Scripts** | `scripts/` | Automated scan tool |

---

## Feedback Principles

### âŒ Bad Feedback
```
"This is wrong"
"LGTM"
"Why did you do this?"
```

### âœ… Good Feedback
```
"[CRIT-001] SQL injection vulnerability at auth.ts:45
Use parameterized queries instead of string concatenation:
cursor.query('SELECT * FROM users WHERE id = ?', [userId])
Reference: OWASP A03:2021"
```

**Good feedback is:**
- Specific (file:line)
- Constructive (suggests solution)
- Explained (why it matters)
- Categorized (severity + category)
- Actionable (clear next step)

---

## Integration Notes

- Use `scripts/auto-scan.py` for quick automated checks
- Follow dimension order (Security first!)
- Always use report template for consistent output
- Review `examples/scenarios/` for real-world patterns

---

## Limitations

- Automated scans are heuristics only - manual review still required
- Cannot detect business logic errors
- Cannot verify security controls effectiveness
- Accessibility review limited to frontend code
- Performance benchmarks require actual load testing
